# LLM-Eval

大语言模型教育领域评估项目

## 项目简介

本项目致力于评估多个主流大语言模型在教育领域的表现，特别是在计算机科学和K-12数学领域的题目生成和回答能力。通过对比分析不同模型的生成质量，为教育技术应用提供参考依据。

## 评估模型

本项目评估了以下6个主流大语言模型：

- **Baichuan（百川）** - 百川智能开源大模型
- **GLM（智谱）** - 智谱AI开发的对话模型  
- **InternLM（书生）** - 上海AI实验室开源大模型
- **Llama** - Meta开源大语言模型
- **Qwen（通义千问）** - 阿里云开发的大语言模型
- **Spark（讯飞星火）** - 科大讯飞星火认知大模型

## 目录结构

```
LLM-Eval-main/
├── dataset/                    # 数据集目录
│   ├── computer/              # 计算机科学相关数据集
│   │   ├── Blooms_Taxonomy.csv    # 布鲁姆分类法数据
│   │   └── Data_Structure.csv     # 数据结构题目数据
│   └── K-12 math/             # K-12数学数据集
│       └── 多知识点题目示例.xlsx    # 多知识点数学题目示例
├── generated/                 # 模型生成结果目录
│   ├── baichuan/             # 百川模型生成结果
│   ├── glm/                  # GLM模型生成结果  
│   ├── internlm/             # InternLM模型生成结果
│   ├── llama/                # Llama模型生成结果
│   ├── qwen/                 # Qwen模型生成结果
│   └── spark/                # Spark模型生成结果
└── README.md                 # 项目说明文档
```

## 文件说明

每个模型的生成结果目录包含以下类型的文件：

- `*_gen_*.xlsx` - 模型生成的题目和答案（Excel格式）
- `*_fine_*.xlsx` - 精调后的模型生成结果
- `*_math_*.xlsx` - 数学相关题目生成结果
- `*_voted.jsonl` - 投票评估结果（JSONL格式）
- `*_generated_questions.xlsx` - 生成的题目汇总

## 评估维度

本项目从以下几个维度评估大语言模型：

1. **题目生成质量** - 评估模型生成教育题目的准确性和合理性
2. **知识覆盖度** - 评估模型对不同知识点的覆盖能力
3. **难度梯度** - 评估模型生成不同难度题目的能力
4. **领域适应性** - 评估模型在计算机科学和数学领域的表现差异

## 使用说明

### 数据集准备

1. 将原始数据集放入 `dataset/` 目录
2. 确保数据格式符合项目要求（CSV/Excel格式）

### 模型评估

1. 运行各个模型的生成脚本
2. 结果会自动保存到对应的 `generated/` 子目录
3. 查看生成的Excel文件和JSONL文件获取评估结果

### 结果分析

- 对比不同模型在同一数据集上的表现
- 分析各模型的优势和不足
- 根据投票结果评估题目质量

## 贡献指南

欢迎为本项目贡献代码和数据：

1. Fork 本项目
2. 创建特性分支
3. 提交更改
4. 发起 Pull Request

## 许可证

本项目采用开源许可证，具体请查看LICENSE文件。

## 联系方式

如有问题或建议，请通过Issue或邮件联系项目维护者。